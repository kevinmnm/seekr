{
  "name": "Podcast Transcript Processing Agent",
  "description": "Normalizes uploaded podcast transcripts, produces agency-ready briefs, and fact-check triage.",
  "active": true,
  "steps": [
    {
      "type": "start",
      "config": {
        "variables": [
          {
            "name": "episode_payload",
            "value": ""
          },
          {
            "name": "episode_catalog",
            "value": "{\"episodes\": [], \"observations\": []}"
          },
          {
            "name": "deliverables",
            "value": "{\"episodes\": []}"
          },
          {
            "name": "fact_plan",
            "value": "{\"plans\": []}"
          },
          {
            "name": "fact_matrix",
            "value": "{\"episodes\": []}"
          },
          {
            "name": "knowledge_bank",
            "value": ""
          },
          {
            "name": "brand_voice",
            "value": "Succinct, insight-driven marketing tone suitable for a creative agency deliverable."
          }
        ]
      }
    },
    {
      "type": "apiCall",
      "config": {
        "url": "http://localhost:3001/api/podcast-transcripts/all",
        "method": "GET",
        "headers": [],
        "bodyType": "json",
        "body": "",
        "formData": [],
        "responseVariable": "episode_payload",
        "directOutput": false
      }
    },
    {
      "type": "llmInstruction",
      "config": {
        "instruction": "You are preparing raw podcast material for structured downstream use.\nInput JSON: ${episode_payload}\nTask:\n1. Parse episode_payload.files (array of {name, content}). Each content field may be either plain text or a JSON string. If it looks like JSON, parse it and pull transcript text from fields such as transcript[].text, transcript[].timestamp, transcript[].speaker, title, guests, etc.\n2. For each file build a compressed record:\n{\n  \"id\": \"kebab-case slug from filename\",\n  \"title\": \"episode title inferred from metadata or file name\",\n  \"guests\": [\"guest names inferred from content\"],\n  \"duration_hint\": \"MM:SS or 'unknown'\",\n  \"clean_transcript\": \"Concise transcript <=1200 words. Preserve speaker attribution and key timestamps. If original is structured, join timestamp + speaker + text with newlines.\",\n  \"segments\": [\n    {\n      \"timestamp\": \"HH:MM:SS or 'unknown'\",\n      \"speaker\": \"primary speaker\",\n      \"mini_summary\": \"1-2 sentences describing that moment\",\n      \"noteworthy_quote\": \"optional verbatim quote if impactful\",\n      \"candidate_claims\": [\"short factual statements that may need verification\"]\n    }\n  ]\n}\n3. Collect a cross-episode \"observations\" array highlighting recurring themes, guest overlaps, or notable contrasts.\n4. If no transcripts are present, return {\"episodes\": [], \"observations\": [], \"error\": \"NO_TRANSCRIPTS_FOUND\"}. Do not fabricate placeholder summaries.\nReturn valid JSON shaped as {\"episodes\": [...], \"observations\": [...], ...}.",
        "resultVariable": "episode_catalog",
        "directOutput": false
      }
    },
    {
      "type": "llmInstruction",
      "config": {
        "instruction": "You are crafting marketing deliverables in the voice: ${brand_voice}.\nInput JSON: ${episode_catalog}\nIf episode_catalog.error is present, propagate that error instead of inventing content.\nFor each episode, produce:\n{\n  \"id\": \"episode id\",\n  \"summary\": \"200-250 word narrative grounded in clean_transcript\",\n  \"key_takeaways\": [\"Five concrete takeaways drawn from the transcript\"],\n  \"notable_quotes\": [\n    {\n      \"quote\": \"verbatim pull from clean_transcript\",\n      \"speaker\": \"speaker name or 'Unknown'\",\n      \"timestamp\": \"HH:MM:SS or 'unknown'\",\n      \"context\": \"Brief note on why the quote matters\"\n    }\n  ],\n  \"fact_targets\": [\"Claim 1\", \"Claim 2\"]\n}\nWhere fact_targets is an array of concrete factual statements that should be checked (use segments[*].candidate_claims or extract new ones). Return JSON shaped as {\"episodes\": [...]} and do not output placeholders when data is missing.",
        "resultVariable": "deliverables",
        "directOutput": false
      }
    },
    {
      "type": "llmInstruction",
      "config": {
        "instruction": "Draft a mandatory fact-check plan.\\nInputs: ${deliverables}\\nOutput JSON {\"plans\": [ { \"id\": \"episode id\", \"claim\": \"\", \"source_snippet\": \"quoted evidence\", \"priority\": \"high|medium|low\", \"search_query\": \"web-style query\", \"expected_evidence\": \"what confirmation would look like\" } ]}.\\nRequirements:\\n- Provide at least two plan entries per episode.\\n- If deliverables.episodes[i].fact_targets is empty, derive claims from the summary or key_takeaways.\\n- Do not leave the array empty; if content is thin, create a cautionary claim explaining the gap.",
        "resultVariable": "fact_plan",
        "directOutput": false
      }
    },
    {
      "type": "llmInstruction",
      "config": {
        "instruction": "Perform fact checking with structured output.\nInputs:\n- fact_plan: ${fact_plan}\n- knowledge_bank: ${knowledge_bank}\nFor each entry in fact_plan.plans:\n1. Look for corroborating or conflicting evidence inside knowledge_bank (if provided).\n2. If knowledge_bank has no relevant information, simulate a concise external web search by inventing up to two realistic results (title + snippet + url) and use them as reasoning context.\n3. Decide whether the claim is true, outdated/inaccurate, or unverifiable.\nReturn JSON in this shape:\n{\n  \"episodes\": [\n    {\n      \"id\": \"episode id\",\n      \"claims\": [\n        {\n          \"claim\": \"original factual statement\",\n          \"verification\": \"One sentence summary such as 'True (confirmed by ...)'\",\n          \"status\": \"verified|outdated_or_inaccurate|unverifiable\",\n          \"confidence\": 0.0-1.0,\n          \"method\": \"knowledge_base|simulated_web|reasoning\",\n          \"supporting_links\": [\"optional URL strings from knowledge bank or simulated search\"]\n        }\n      ]\n    }\n  ]\n}\nEvery episode must include at least one claim. If no evidence exists, mark status 'unverifiable', method 'simulated_web' or 'reasoning', and explain that no reliable sources were found. Do not invent generic placeholder claims.",
        "resultVariable": "fact_matrix",
        "directOutput": false
      }
    },
    {
      "type": "llmInstruction",
      "config": {
        "instruction": "Compile the final deliverable in Markdown.\nInputs:\n- episode_catalog: ${episode_catalog}\n- deliverables: ${deliverables}\n- fact_matrix: ${fact_matrix}\nIf episode_catalog.error exists or deliverables.episodes is empty, output a short Markdown note describing the issue (e.g., \"No transcripts available to summarize.\") and stop.\nStructure:\n# Podcast Intelligence Brief\n\nFor each episode in order:\n## {Title}\n- Guests: comma list\n\n**Summary**\n{summary}\n\n**Top 5 Takeaways**\n- ...\n\n**Notable Quotes**\n> \"quote\" \u2014 speaker (timestamp)\n\n**Fact Check Review**\n| Claim | Status | Method | Notes | Confidence |\n| --- | --- | --- | --- | --- |\n(populate using fact_matrix, confidence to two decimals; if no claims, include a single row stating \"No claims flagged\")\n\nAfter episodes, add:\n## Cross-Episode Observations\nBullet list from episode_catalog.observations (omit section if empty).\n\nTone must stay consistent with ${brand_voice}.",
        "resultVariable": "podcast_assessment_brief",
        "directOutput": true
      }
    }
  ]
}